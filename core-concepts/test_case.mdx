---
title: "Test Cases"
description: "Creating and running test cases to ensure agent quality"
---

## What is a Test Case?

A **test case** is a blueprint for testing your agent. It contains a recorded session that serves as the "golden standard" and defines the expected behavior your agent should consistently meet.

Test cases turn one-time agent executions into repeatable quality checks that catch regressions before they reach production.

<Note>
  Think of test cases as unit tests for agent behavior, not just code
  correctness
</Note>

## Creating Test Cases

### Step 1: Choose a Test Scenario

Choose a test scenario that you want to test. This could be a specific user interaction, a specific error condition, or a specific feature.

For example, you might want to test the following:

- Flight Booking Agent: A user booking a flight
- Browser Agent: A user searching for available apartments in a city
- Todo List Agent: A user adding and retrieving tasks from a todo list

### Step 2: Create Test Case

Navigate to the Playgent dashboard and:

1. Go to the "Tests" section
2. Click "Create Test Case"
3. Describe the test scenario in the description field

<img
  style={{ borderRadius: "0.5rem" }}
  src="/images/createtest.gif"
  alt="Creating a test case in Playgent dashboard"
/>

### Step 3: Record a Demonstration Session

You don't need to write any special code to record a demonstration session. Simply use your agent normally - as long as the `@record` decorator is on your agent's entrypoint function, Playgent will automatically track all LLM calls and create a session.

- If you have a **frontend application**, you can demonstrate the run directly through your UI
- If your agent uses **external APIs or databases**, the real execution will capture authentic interactions
- If there's **complex state management**, using the actual system ensures accurate recording

Just run your agent with the behavior you want to test, and Playgent captures everything automatically.

<Warning>
  For **multi-turn interactions**, make sure all turns use the same session ID
  to group their events together. Use the `session()` context manager or
  `set_session()` to maintain session continuity across multiple calls.
</Warning>

### Step 4: Annotate the Demonstration Session

Once you have a successful session, you can link this to the test case you created. You can also annotate key behaviors here. These annotations will be used to generate a rubric for the test case.

<img
  style={{ borderRadius: "0.5rem" }}
  src="/images/annotatesession.png"
  alt="Annotating a session in Playgent dashboard"
/>

<Tip>
  Annotations help generate a detailed rubric that makes evaluation more
  accurate
</Tip>

**And that's it! You've created a test case for your agent.**

## Running Test Cases

### Using replay_test

The `replay_test` context manager is the main way to run test cases. It provides:

- A new session that will be linked to the test case (e.g. `flight-seat-change-test`) on Playgent dashboard
- The exact inputs to the agent function from the demonstration session associated with the test case
- A judge object that can be used to evaluate the test case results against the rubric

You are responsible for calling the agent function with these inputs.

```python
from playgent import replay_test
from example_agent import infer

def test_flight_seat_change():
    with replay_test("flight-seat-change-test") as (inputs, judge):
        # Replay each call to `infer` from the recorded session
        for inp in inputs:
            infer(**inp.arguments)

        # Evaluate the results against the rubric
        result = judge.evaluate()
        assert result.passed
```

<Note>
  Evaluation uses LLM-as a judge to evaluate the test case results against the
  rubric.
</Note>

In some cases, you may want to mock or stub out certain inputs to the agent function. You have the flexibility to define how you run the test.

```python
from playgent import replay_test
from example_agent import infer

def test_flight_date():
    with replay_test("flight-date-test") as (inputs, judge):
        for inp in inputs:
            # Replace the old timestamp with the current time
            inp.arguments["flight_date"] = get_current_time()
            infer(**inp.arguments)
        result = judge.evaluate()
        assert result.passed
```

You can even define your own custom logic to run the test.

```python
def test_book_cancel_ticket():
    with replay_test("flight-book-cancel-ticket") as (_, judge):
        infer(input_text="Book a ticket for San Francisco to New York on October 27, 2025")
        infer(input_text="Can you cancel the ticket?")
        result = judge.evaluate()
        assert result.passed
```

### Integration with Test Runners

Works with any Python test framework:

#### pytest

```python
import pytest
from playgent import replay_test
from my_agent import booking_agent

@pytest.mark.parametrize("test_id", [
    "flight-booking-test",
    "flight-cancellation-test",
    "seat-change-test"
])
def test_agent_behavior(test_id):
    with replay_test(test_id) as (inputs, judge):
        for event in inputs:
            booking_agent(**event.arguments)

        result = judge.evaluate()
        assert result.passed
```

#### unittest

```python
import unittest
from playgent import replay_test
from my_agent import booking_agent

class TestAgentBehavior(unittest.TestCase):
    def test_flight_booking(self):
        with replay_test("flight-booking-test") as (inputs, judge):
            for event in inputs:
                booking_agent(**event.arguments)

            result = judge.evaluate()
            self.assertTrue(result.passed)
```

## Test Case Management

### Organizing Tests

Use tags and naming conventions:

```python
# Group related tests
with replay_test("booking-happy-path") as (inputs, judge):
    ...

with replay_test("booking-edge-case-invalid-date") as (inputs, judge):
    ...

with replay_test("booking-error-handling-sold-out") as (inputs, judge):
    ...
```

### Updating Test Cases

When expected behavior changes:

1. Record a new demonstration session with correct behavior
2. Annotate the new session and link it to the test case
3. Tests will now use the new demonstration session

## CI/CD Integration

Run tests in your continuous integration pipeline:

```yaml
# .github/workflows/test.yml
name: Test Agent Behavior

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install playgent pytest
          pip install -r requirements.txt

      - name: Run Playgent tests
        env:
          PLAYGENT_API_KEY: ${{ secrets.PLAYGENT_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: pytest tests/test_agent.py
```

<Tip>
  Store your API keys as secrets in your CI environment for secure testing
</Tip>

## Common Patterns

### Testing Different Scenarios

```python
# Happy path
def test_booking_success():
    with replay_test("booking-success") as (inputs, judge):
        for event in inputs:
            booking_agent(**event.arguments)
        assert judge.evaluate().passed

# Edge case
def test_booking_invalid_date():
    with replay_test("booking-invalid-date") as (inputs, judge):
        for event in inputs:
            booking_agent(**event.arguments)
        assert judge.evaluate().passed

# Error handling
def test_booking_sold_out():
    with replay_test("booking-sold-out") as (inputs, judge):
        for event in inputs:
            booking_agent(**event.arguments)
        assert judge.evaluate().passed
```

<CardGroup cols={2}>
  <Card title="Sessions" icon="layer-group" href="/core-concepts/session">
    Learn how to create baseline sessions
  </Card>
  <Card title="Todo List Example" icon="code" href="/examples/todo-list">
    See a complete example with tests
  </Card>
</CardGroup>
