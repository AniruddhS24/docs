---
title: "Test Cases"
description: "Creating and running test cases to ensure agent quality"
---

## What is a Test Case?

A **test case** is a blueprint for testing your agent. It contains a recorded session that serves as the "golden standard" and defines the expected behavior your agent should consistently meet.

Test cases turn one-time agent executions into repeatable quality checks that catch regressions before they reach production.

<Note>
  Think of test cases as unit tests for agent behavior, not just code
  correctness
</Note>

## Creating Test Cases

### Step 1: Choose a Test Scenario

Choose a test scenario that you want to test. This could be a specific user interaction, a specific error condition, or a specific feature.

For example, you might want to test the following:

- Flight Booking Agent: A user booking a flight
- Browser Agent: A user searching for available apartments in a city
- Todo List Agent: A user adding and retrieving tasks from a todo list

### Step 2: Create Test Case

Navigate to the Playgent dashboard and:

1. Go to the "Tests" section
2. Click "Create Test Case"
3. Describe the test scenario in the description field

<img
  style={{ borderRadius: "0.5rem" }}
  src="/images/createtest.gif"
  alt="Creating a test case in Playgent dashboard"
/>

### Step 3: Record a Demonstration Session

You don't need to write any special code to record a demonstration session. Simply use your agent normally - as long as the `@record` decorator is on your agent's entrypoint function, Playgent will automatically track all LLM calls and create a session.

- If you have a **frontend application**, you can demonstrate the run directly through your UI
- If your agent uses **external APIs or databases**, the real execution will capture authentic interactions
- If there's **complex state management**, using the actual system ensures accurate recording

Just run your agent with the behavior you want to test, and Playgent captures everything automatically.

<Warning>
  For **multi-turn interactions**, make sure all turns use the same session ID
  to group their events together. Use the `session()` context manager or
  `set_session()` to maintain session continuity across multiple calls.
</Warning>

### Step 4: Annotate the Demonstration Session

Once you have a successful session, you can link this to the test case you created. You can also annotate key behaviors here. These annotations will be used to generate a rubric for the test case.

<img
  style={{ borderRadius: "0.5rem" }}
  src="/images/annotatesession.png"
  alt="Annotating a session in Playgent dashboard"
/>

<Tip>
  Annotations help generate a detailed rubric that makes evaluation more
  accurate
</Tip>

**And that's it! You've created a test case for your agent.**

## Running Test Cases

### Using replay_test

The `replay_test` context manager is the main way to run test cases. It will replay the inputs from the demonstration session with all of the arguments passed to the agent function.

The `inputs` variable is a list of all the captured arguments to the agent function during the demonstration session. You are responsible for calling the agent function with these arguments.

In simple cases, you can just pass all the arguments directly to the agent function. However, you have the flexibility to only pass some arguments, and mock or stub out the others.

You can also use the `judge` object to evaluate the results of the test case based on the generated rubric.

```python
from playgent import replay_test
from my_agent import booking_agent

def test_flight_booking():
    with replay_test("flight-booking-test") as (inputs, judge):
        # Replay all inputs from the baseline session
        for event in inputs:
            booking_agent(**event.arguments)

        # Evaluate against the rubric
        result = judge.evaluate()

        # Assert the test passed
        assert result.passed, f"Test failed: {result.summary}"
```

### Integration with Test Runners

Works with any Python test framework:

#### pytest

```python
import pytest
from playgent import replay_test
from my_agent import booking_agent

@pytest.mark.parametrize("test_id", [
    "flight-booking-test",
    "flight-cancellation-test",
    "seat-change-test"
])
def test_agent_behavior(test_id):
    with replay_test(test_id) as (inputs, judge):
        for event in inputs:
            booking_agent(**event.arguments)

        result = judge.evaluate()
        assert result.passed
```

#### unittest

```python
import unittest
from playgent import replay_test
from my_agent import booking_agent

class TestAgentBehavior(unittest.TestCase):
    def test_flight_booking(self):
        with replay_test("flight-booking-test") as (inputs, judge):
            for event in inputs:
                booking_agent(**event.arguments)

            result = judge.evaluate()
            self.assertTrue(result.passed)
```

<Note>
  Evaluation uses advanced LLM-as a judge to evaluate the test case results
  against the rubric.
</Note>

## Test Case Management

### Organizing Tests

Use tags and naming conventions:

```python
# Group related tests
with replay_test("booking-happy-path") as (inputs, judge):
    ...

with replay_test("booking-edge-case-invalid-date") as (inputs, judge):
    ...

with replay_test("booking-error-handling-sold-out") as (inputs, judge):
    ...
```

### Updating Test Cases

When expected behavior changes:

1. Record a new demonstration session with correct behavior
2. Annotate the new session and link it to the test case
3. Tests will now use the new demonstration session

### Versioning

Test cases are versioned automatically:

- Each update creates a new version
- You can compare versions in the dashboard
- Roll back to previous versions if needed

## CI/CD Integration

Run tests in your continuous integration pipeline:

```yaml
# .github/workflows/test.yml
name: Test Agent Behavior

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install playgent pytest
          pip install -r requirements.txt

      - name: Run Playgent tests
        env:
          PLAYGENT_API_KEY: ${{ secrets.PLAYGENT_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: pytest tests/test_agent.py
```

<Tip>
  Store your API keys as secrets in your CI environment for secure testing
</Tip>

## Common Patterns

### Testing Different Scenarios

```python
# Happy path
def test_booking_success():
    with replay_test("booking-success") as (inputs, judge):
        for event in inputs:
            booking_agent(**event.arguments)
        assert judge.evaluate().passed

# Edge case
def test_booking_invalid_date():
    with replay_test("booking-invalid-date") as (inputs, judge):
        for event in inputs:
            booking_agent(**event.arguments)
        assert judge.evaluate().passed

# Error handling
def test_booking_sold_out():
    with replay_test("booking-sold-out") as (inputs, judge):
        for event in inputs:
            booking_agent(**event.arguments)
        assert judge.evaluate().passed
```

### Parameterized Tests

```python
@pytest.mark.parametrize("test_name,user_id,min_score", [
    ("booking-success", "user_123", 90),
    ("booking-cancellation", "user_456", 85),
    ("seat-change", "user_789", 80)
])
def test_agent_quality(test_name, user_id, min_score):
    with replay_test(test_name) as (inputs, judge):
        for event in inputs:
            booking_agent(**event.arguments)

        result = judge.evaluate()
        assert result.score >= min_score
```

<CardGroup cols={2}>
  <Card title="Sessions" icon="layer-group" href="/core-concepts/session">
    Learn how to create baseline sessions
  </Card>
  <Card title="Todo List Example" icon="code" href="/examples/todo-list">
    See a complete example with tests
  </Card>
</CardGroup>
